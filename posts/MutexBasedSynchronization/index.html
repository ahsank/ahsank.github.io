<!doctype html> <html lang=en > <meta charset=UTF-8 > <meta name=viewport  content="width=device-width, initial-scale=1"> <script async src="https://www.googletagmanager.com/gtag/js?id=G-60YTC6HS65"></script> <script> window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-60YTC6HS65'); </script> <link rel=stylesheet  href="/libs/highlight/styles/github.min.css"> <link rel=stylesheet  href="/css/franklin.css"> <link rel=stylesheet  href="/css/basic.css"> <link rel=icon  href="/assets/favicon.png"> <title>Mutex-Based Synchronization: Understanding Performance Impacts</title> <header> <div class=blog-name ><a href="/">Ahsan Khan</a></div> <nav> <ul> <li><a href="/">Main</a> <li><a href="/about//">About</a> </ul> <img src="/assets/hamburger.svg" id=menu-icon > </nav> </header> <div class=franklin-content ><h1 id=mutex-based_synchronization_understanding_performance_impacts ><a href="#mutex-based_synchronization_understanding_performance_impacts" class=header-anchor >Mutex-Based Synchronization: Understanding Performance Impacts</a></h1> <p>When designing a high-performance systems, an early decision is to select the appropriate memory model and synchronization strategy. Consider a simple multithreaded cache manager with a single interface: <code>get&#40;key&#41; -&gt; value</code>, where the key represents the file name and the value represents its content. Here the <code>get</code> operation checks if the entry is present in the hash table. If not, then loads the content from disk, inserts it into the hash table, and then returns the content.</p> <p>Even for this basic system, synchronization of access to the hash table is necessary. The pseudocode for such a system can be summarized as follows:</p> <ol> <li><p>Check if the entry is present in the cache by</p> <ul> <li><p>Locking a mutex</p> <li><p>Checking if the entry is present in the hash table</p> <li><p>Unlocking the mutex</p> </ul> <li><p>If the entry was present, go to step 4. Otherwise, read the value from disk &#40;possibly in a separate thread or thread pool&#41;.</p> <li><p>Add the value to the cache</p> <ul> <li><p>Lock the mutex</p> <li><p>Add the value to the hash table</p> <li><p>Unlock the mutex</p> </ul> <li><p>Return the value</p> </ol> <h2 id=benchmarking_mutex_locking_performance ><a href="#benchmarking_mutex_locking_performance" class=header-anchor >Benchmarking Mutex Locking Performance</a></h2> <p>To simulate a similar pattern, we can perform the following operations:</p> <ol> <li><p>Lock a mutex and perform CPU-intensive operations on a data structure, then unlock the mutex.</p> <li><p>Perform IO/network operations that can be parallelized.</p> <li><p>Lock the mutex again and perform additional operations on the data structure before unlocking the mutex.</p> </ol> <p>In this blog, I will do benchmarks to find how mutex locking affects performance. Subsequently, I plan to evaluate different libraries and alternatives of locking.</p> <h3 id=simulating_io_operations_for_benchmarking ><a href="#simulating_io_operations_for_benchmarking" class=header-anchor >Simulating IO Operations for Benchmarking</a></h3> <p>The benchmark time heavily depends on the IO library and the intricacies of the computer system. In this blog, my primary goal is not to compare different IO libraries; rather, to measure the performance impact of the locking mechanism. To simplify simulating IO operations I initially thought of just using the sleep function. However then found that the minimum time duration the API could reliably sleep &#40;without oversleeping&#41; was approximately 1 millisecond, which seems impractical for mimicking realistic loads.</p> <p>To determine a more realistic IO time, I did an experiment writing an 8KB memory to disk, which completed in less than 10 microseconds. Reading from disk was even faster. Therefore an IO time ranging from 10 to 100 microseconds seemed reasonable for our benchmarking purposes.</p> <p>To exemplify, if I benchmark the following function which just sleeps for 8 and 64 microseconds:</p> <pre><code class="cpp hljs"><span class=hljs-keyword >struct</span> <span class="hljs-title class_">sleep_io</span> {
    <span class=hljs-function ><span class=hljs-type >static</span> <span class=hljs-keyword >inline</span> <span class=hljs-type >void</span> <span class=hljs-title >fake_io</span><span class=hljs-params >(std::chrono::microseconds sleep_time)</span> </span>{
        std::this_thread::<span class=hljs-built_in >sleep_for</span>(sleep_time);
    }
};

<span class=hljs-function ><span class=hljs-type >void</span> <span class=hljs-title >BM_sleep</span><span class=hljs-params >(benchmark::State&amp; state)</span> </span>{
    <span class=hljs-keyword >auto</span> duration = std::chrono::<span class=hljs-built_in >microseconds</span>(state.<span class=hljs-built_in >range</span>(<span class=hljs-number >0</span>));
    <span class=hljs-keyword >for</span> (<span class=hljs-keyword >auto</span> _ : state) {
        sleep_io::<span class=hljs-built_in >fake_io</span>(duration);
    }
}

<span class=hljs-built_in >BENCHMARK</span>(BM_sleep)-&gt;<span class=hljs-built_in >Unit</span>(benchmark::kMicrosecond)-&gt;<span class=hljs-built_in >Range</span>(<span class=hljs-number >8</span>, <span class=hljs-number >64</span>);</code></pre> <p>The benchmark results indicate that the actual time slept was considerably larger than the requested durations.</p> <table><tr><th align=right >Benchmark<th align=right >Time<th align=right >CPU<th align=right >Iterations<tr><td align=right ><code>BM_sleep/8</code><td align=right >83.6 us<td align=right >4.25 us<td align=right >100000<tr><td align=right ><code>BM_sleep/64</code><td align=right >154 us<td align=right >4.41 us<td align=right >100000</table> <p>This prompted me to develop a custom fake_io operation operation. It just reads from <code>/dev/zero</code> so will only work in Linux and Mac OSX.</p> <pre><code class="julia hljs">// More accurate sleep
<span class=hljs-keyword >struct</span> devzero_io {
    static inline void fake_io(std::chrono::microseconds sleep_time) {
        <span class=hljs-keyword >if</span> (sleep_time &lt;= <span class=hljs-number >0</span>us) <span class=hljs-keyword >return</span>;
        <span class=hljs-keyword >using</span> clock =  std::chrono::steady_clock;
        auto end_time = clock::now() + sleep_time;
        std::ifstream zfs(<span class=hljs-string >&quot;/dev/zero&quot;</span>);
        <span class=hljs-keyword >const</span> int readsize = <span class=hljs-number >1024</span>;
        char v[readsize];
        <span class=hljs-keyword >while</span> ( clock::now() &lt; end_time) {
            zfs.read(v, readsize);
        }
        zfs.close();
    }
};

<span class=hljs-keyword >using</span> iotype = devzero_io;</code></pre> <p>Benchmarking this simulated IO operation, I got results close to the specified parameters, validating its effectiveness.</p> <pre><code class="julia hljs">void BM_sleep_devzero(benchmark::State&amp; state) {
    auto duration = std::chrono::microseconds(state.range(<span class=hljs-number >0</span>));
    <span class=hljs-keyword >for</span> (auto _ : state) {
        iotype::fake_io(duration);
    }
}

BENCHMARK(BM_sleep_devzero)-&gt;Unit(benchmark::kMicrosecond)-&gt;Range(<span class=hljs-number >8</span>, <span class=hljs-number >64</span>);</code></pre> <table><tr><th align=right >Benchmark<th align=right >Time<th align=right >CPU<th align=right >Iterations<tr><td align=right ><code>BM_sleep_devzero/8</code><td align=right >8.91 us<td align=right >8.87 us<td align=right >75051<tr><td align=right ><code>BM_sleep_devzero/64</code><td align=right >64.8 us<td align=right >64.8 us<td align=right >10818</table> <p>After determining the strategy for IO operation simulation, I used a CPU-intensive operation that increments an integer value and saves the value in a hash table as a string. This operation is representative of scenarios where data structure synchronization is necessary.</p> <pre><code class="cpp hljs"><span class=hljs-keyword >using</span> cachetype = std::unordered_map&lt;std::string, std::string&gt;;
<span class=hljs-type >const</span> <span class=hljs-type >unsigned</span> <span class=hljs-type >int</span> max_iter = <span class=hljs-number >1000</span>;
<span class=hljs-type >const</span> std::string cache_key = <span class=hljs-string >&quot;hello&quot;</span>;
<span class=hljs-type >const</span> <span class=hljs-type >unsigned</span> <span class=hljs-type >int</span> num_calc = <span class=hljs-number >10</span>;
<span class=hljs-keyword >constexpr</span> <span class=hljs-type >int</span> start_val = <span class=hljs-number >10000</span>;

<span class=hljs-comment >// Simulate CPU operation by just incrementing a string value</span>
<span class=hljs-comment >// in a hash table</span>
<span class=hljs-function ><span class=hljs-keyword >inline</span> std::string <span class=hljs-title >get_key</span><span class=hljs-params >(<span class=hljs-type >int</span> i)</span> </span>{<span class=hljs-keyword >return</span> cache_key + std::<span class=hljs-built_in >to_string</span>(i);}

<span class=hljs-function ><span class=hljs-keyword >inline</span> <span class=hljs-type >void</span> <span class=hljs-title >calc</span><span class=hljs-params >(cachetype&amp; cache)</span> </span>{
    <span class=hljs-comment >// Some complex calculations</span>
    <span class=hljs-keyword >for</span> (<span class=hljs-type >int</span> i=<span class=hljs-number >0</span>; i &lt; num_calc; i++) {
        <span class=hljs-keyword >auto</span> key = <span class=hljs-built_in >get_key</span>(i);
        <span class=hljs-keyword >auto</span> str = cache[key];
        <span class=hljs-comment >// converts to int, increments and converts back to string</span>
        cache[key] = str.<span class=hljs-built_in >length</span>() == <span class=hljs-number >0</span> ?
            std::<span class=hljs-built_in >to_string</span>(start_val+<span class=hljs-number >1</span>) : std::<span class=hljs-built_in >to_string</span>(std::<span class=hljs-built_in >stoi</span>(str) + <span class=hljs-number >1</span>);
    }
}</code></pre> <p>Following is the code for the single threaded lock-free version of the benchmark:</p> <pre><code class="cpp hljs"><span class=hljs-function ><span class=hljs-type >static</span> <span class=hljs-type >void</span> <span class=hljs-title >cache_calc</span><span class=hljs-params >(cachetype&amp; cache,
    std::chrono::duratin sleep_time)</span> </span>{
    <span class=hljs-keyword >for</span> (<span class=hljs-type >int</span> i=<span class=hljs-number >0</span>; i &lt; max_iter; i++) {
        <span class=hljs-built_in >calc</span>(cache);
        iotype::<span class=hljs-built_in >fake_io</span>(sleep_time);
        <span class=hljs-built_in >calc</span>(cache);
    }
}</code></pre> <p>Here, <code>sleep_time</code> simulates the I/O intensity of the application, and we will perform the benchmark with values of 8us and 64us.</p> <p>The benchmark results for 1000 iterations:</p> <table><tr><th align=right >Benchmark<th align=right >Time<th align=right >CPU<th align=right >Iterations<tr><td align=right ><code>BM_cachecalc/8</code><td align=right >10.1 ms<td align=right >10.0 ms<td align=right >68<tr><td align=right ><code>BM_cachecalc/64</code><td align=right >65.8 ms<td align=right >65.8 ms<td align=right >11</table> <p>Adding mutex in a single-threaded program surprisingly shows no overhead, indicating minimal impact when there is no contention:</p> <pre><code class="cpp hljs"><span class=hljs-function ><span class=hljs-type >void</span> <span class=hljs-title >cache_calc_mutex</span><span class=hljs-params >(cachetype&amp; cache, std::chrono::microseconds sleep_time)</span> </span>{
    std::mutex m;
    <span class=hljs-keyword >for</span> (<span class=hljs-type >int</span> i=<span class=hljs-number >0</span>; i &lt; max_iter; i++) {
        {
            <span class=hljs-function >std::scoped_lock <span class=hljs-title >lck</span><span class=hljs-params >(m)</span></span>;
            <span class=hljs-built_in >calc</span>(cache);
        }
        iotype::<span class=hljs-built_in >fake_io</span>(sleep_time);
        {
            <span class=hljs-function >std::scoped_lock <span class=hljs-title >lck</span><span class=hljs-params >(m)</span></span>;
            <span class=hljs-built_in >calc</span>(cache);
        }
    }
}</code></pre> <p>Benchmark results with mutex:</p> <table><tr><th align=right >Benchmark<th align=right >Time<th align=right >CPU<th align=right >Iterations<tr><td align=right ><code>BM_cachecalc_mutex/8</code><td align=right >10.3 ms<td align=right >10.2 ms<td align=right >68<tr><td align=right ><code>BM_cachecalc_mutex/64</code><td align=right >66.7 ms<td align=right >66.7 ms<td align=right >11</table> <p>The code and benchmark results for distributing work among 8 threads are as follows:</p> <pre><code class="cpp hljs"><span class=hljs-keyword >constexpr</span> <span class=hljs-type >unsigned</span> <span class=hljs-type >int</span> num_tasks = <span class=hljs-number >8</span>

<span class=hljs-type >static</span> <span class=hljs-type >void</span> <span class=hljs-built_in >cache_calc_async</span>(cachetype&amp; map, std::chrono::microseconds sleep_time) {
    std::queue&lt;std::future&lt;<span class=hljs-type >void</span>&gt;&gt; futures;
    std::mutex m;

    <span class=hljs-keyword >for</span> (<span class=hljs-type >int</span> i=<span class=hljs-number >0</span>; i &lt; max_iter; i++) {
        futures.<span class=hljs-built_in >push</span>(std::<span class=hljs-built_in >async</span>([&amp;]{
            {
                std::scoped_lock <span class=hljs-built_in >lck</span>(m);
                <span class=hljs-built_in >calc</span>(map);
            }
            iotype::<span class=hljs-built_in >fake_io</span>(sleep_time);
            {
                std::scoped_lock <span class=hljs-built_in >lck</span>(m);
                <span class=hljs-built_in >calc</span>(map);
            }
        }));
        <span class=hljs-comment >// Make sure no more parallel tasks than num_tasks</span>
        <span class=hljs-keyword >while</span> (futures.<span class=hljs-built_in >size</span>() &gt;= num_tasks) {
            futures.<span class=hljs-built_in >front</span>().<span class=hljs-built_in >wait</span>();
            futures.<span class=hljs-built_in >pop</span>();
        }
    }
    <span class=hljs-keyword >while</span>(!futures.<span class=hljs-built_in >empty</span>()) {
        futures.<span class=hljs-built_in >front</span>().<span class=hljs-built_in >wait</span>();
        futures.<span class=hljs-built_in >pop</span>();
    }
}</code></pre> <p>Benchmark results for 8 threads:</p> <table><tr><th align=right >Benchmark<th align=right >Time<th align=right >CPU<th align=right >Iterations<tr><td align=right ><code>BM_cachecalc_async/8</code><td align=right >10.0 ms<td align=right >9.66 ms<td align=right >66<tr><td align=right ><code>BM_cachecalc_async/64</code><td align=right >23.6 ms<td align=right >12.0 ms<td align=right >61</table> <p>For the full code of the benchmark, you can visit <a href="https://github.com/ahsank/EvaluateIPC/blob/master/tests/mutexbench.cc">EvaluateIPC GitHub Repository</a></p> <script src="https://utteranc.es/client.js" repo="ahsank/ahsank.github.io" issue-term=pathname  label=Comment  theme=github-light  crossorigin=anonymous  async> </script> <div class=page-foot > <a href="http://creativecommons.org/licenses/by-sa/4.0/">CC BY-SA 4.0</a> Ahsan Khan. Last modified: January 09, 2024. Website built with <a href="https://github.com/tlienart/Franklin.jl">Franklin.jl</a>. </div> </div> <script src="/libs/highlight/highlight.min.js"></script> <script>hljs.highlightAll();hljs.configure({tabReplace: ' '});</script>